{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danishnaseer00/codeminx/blob/main/Fine_Tuning_with_Unsloth_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfRZYLpHCtNd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install transformers datasets peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7U6NHLK9CuS_"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling , EarlyStoppingCallback\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0bE60-3EAZD"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub\n",
        "print(huggingface_hub.utils.get_session().get(\"https://huggingface.co\").status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8m6baX_EHF9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "    from google.colab import userdata\n",
        "\n",
        "    HF_TOKEN = userdata.get('HF-TOKEN')\n",
        "\n",
        "    if HF_TOKEN:\n",
        "        login(token=HF_TOKEN)\n",
        "        print(\"Successfully logged in to Hugging Face!\")\n",
        "    else:\n",
        "        print(\"Hugging Face token not found in Colab Secrets. Please add it.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxH2HSXiDGbB"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 1024\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name= \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\"]\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "1wapXqRY4Qbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8UGEaMCDitV"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a25ZQ0q3nqh"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\", split=\"train\").shuffle(seed=3407).select(range(7000))\n",
        "dataset = dataset.train_test_split(test_size=0.15, seed=3407)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ssTwFBNZ4d4r"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZYs8VZJ3IUZE"
      },
      "outputs": [],
      "source": [
        "def format_prompts(examples):\n",
        "    \"\"\"\n",
        "    Format the dataset for instruction tuning with Phi-3 Mini template\n",
        "    \"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    texts = []\n",
        "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
        "        try:\n",
        "            if input_text and input_text.strip():\n",
        "                prompt = f\"\"\"<|user|>\\n{instruction}\\nInput: {input_text}<|end|>\\n<|assistant|>\\n{output}<|end|>\"\"\"\n",
        "            else:\n",
        "                prompt = f\"\"\"<|user|>\\n{instruction}<|end|>\\n<|assistant|>\\n{output}<|end|>\"\"\"\n",
        "            texts.append(prompt)\n",
        "        except Exception as e:\n",
        "            print(f\"Error formatting prompt: {e}\")\n",
        "            texts.append(\"\")\n",
        "    return {\"text\": texts}\n",
        "\n",
        "train_dataset = dataset[\"train\"].map(format_prompts, batched=True)\n",
        "eval_dataset = dataset[\"test\"].map(format_prompts, batched=True)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_seq_length)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulkOQV3FIvjh"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./Fine_tuned_LLM\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_steps=100,\n",
        "    learning_rate=1e-5,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sreYgOgrJFsz"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=5,\n",
        "            early_stopping_threshold=0.001\n",
        "        )\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cKuFUZdgJOWT"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(\"Starting training...\")\n",
        "print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m86cdngnJR1s"
      },
      "outputs": [],
      "source": [
        "def validate_model(prompt, max_length=512):\n",
        "    \"\"\"Test the model with a sample prompt\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\\n\",\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            tokenizer = tokenizer,\n",
        "\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "\n",
        "            stop_strings=[\"```\\n\", \"### Instruction:\", \"### Input:\", \"### Response:\"]\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "# Test the model\n",
        "test_prompt = \"Write function to check a string is palindrome or not \"\n",
        "print(\"\\nTest Generation:\")\n",
        "print(validate_model(test_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RVyto2EKu7i"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./Fine_tuned_LLM\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    logging_steps=1,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=5,\n",
        "            early_stopping_threshold=0.001\n",
        "        )\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "gwtQEwulLzFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "print(\"Starting training...\")\n",
        "print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=\"/content/Fine_tuned_LLM/checkpoint-100\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"GPU memory after training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
      ],
      "metadata": {
        "id": "YD51fUX4L2aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(prompt, max_length=512):\n",
        "    \"\"\"Test the model with a sample prompt\"\"\"\n",
        "    inputs = tokenizer(\n",
        "        f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\\n\",\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            tokenizer = tokenizer,\n",
        "\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "\n",
        "            stop_strings=[\"```\\n\", \"### Instruction:\", \"### Input:\", \"### Response:\"]\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "# Test the model\n",
        "test_prompt = \"Write a mergesort algorithm in python.\"\n",
        "print(\"\\nTest Generation:\")\n",
        "print(validate_model(test_prompt))"
      ],
      "metadata": {
        "id": "gbX6GrAYMrge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"phi3-python-fine_tuned\")\n",
        "tokenizer.save_pretrained(\"phi3-python-fine_tuned\")"
      ],
      "metadata": {
        "id": "ArdkdNDGQUVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git"
      ],
      "metadata": {
        "id": "XoodHXscb8hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"danishnaseer00\"\n",
        "!git config --global user.email \"danishmughal.dev@gmail.com\""
      ],
      "metadata": {
        "id": "AyqPnEpkgBMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Fetch the token from Colab secrets\n",
        "try:\n",
        "    GITHUB_TOKEN = userdata.get('GH-token')\n",
        "    print(\"Token fetched successfully (length: {} characters)\".format(len(GITHUB_TOKEN)))\n",
        "except Exception as e:\n",
        "    print(f\"Error fetching token: {e}\")\n",
        "    print(\"Make sure you added the secret named 'GITHUB_TOKEN' in Colab secrets.\")\n",
        "    GITHUB_TOKEN = None"
      ],
      "metadata": {
        "id": "BbR3vtJBgJwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Set up GitHub repository details\n",
        "REPO_URL=\"https://github.com/danishnaseer00/codeminx.git\"\n",
        "BRANCH=\"main\"\n",
        "\n",
        "# Create a new directory for the repo\n",
        "mkdir Fine_tuned_phi\n",
        "cd Fine_tuned_phi # Changed from my_model to Fine_tuned_phi\n",
        "\n",
        "# Initialize git\n",
        "git init\n",
        "git checkout -b $BRANCH\n",
        "\n",
        "# Copy all files from Colab environment (adjust paths as needed)\n",
        "cp -r /content/* .\n",
        "\n",
        "# Add all files including notebook, model, and tokenizer\n",
        "git add .\n",
        "git commit -m \"Upload Colab notebook, fine-tuned model, and tokenizer\"\n",
        "\n",
        "# Add GitHub remote and push\n",
        "git remote add origin $REPO_URL\n",
        "git push -u origin $BRANCH"
      ],
      "metadata": {
        "id": "y9zl2jeVe72G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nbformat==5.10.4 nbconvert==7.16.6"
      ],
      "metadata": {
        "id": "mXECPtL5fdxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade jupyter notebook nbformat nbconvert"
      ],
      "metadata": {
        "id": "gffUGxH1i_1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZJFRVJVjZGW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}